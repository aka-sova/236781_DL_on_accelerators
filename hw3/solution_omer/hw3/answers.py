r"""
Use this module to write your answers to the questions in the notebook.

Note: Inside the answer strings you can use Markdown format and also LaTeX
math (delimited with $$).
"""

# ==============
# Part 1 answers

def part1_rnn_hyperparams():
    hypers = dict(
        batch_size=0, seq_len=0,
        h_dim=0, n_layers=0, dropout=0,
        learn_rate=0.0, lr_sched_factor=0.0, lr_sched_patience=0,
    )
    # TODO: Set the hyperparameters to train the model.
    # ====== YOUR CODE: ======
    
    hypers['batch_size']        = 512
    hypers['seq_len']           = 64
    hypers['h_dim']             = 256
    hypers['n_layers']          = 5
    hypers['dropout']           = 0.3
    hypers['learn_rate']        = 0.001
    hypers['lr_sched_factor']   = 0.3
    hypers['lr_sched_patience'] = 10


    # raise NotImplementedError()
    # ========================
    return hypers


def part1_generation_params():
    start_seq = ""
    temperature = .0001
    # TODO: Tweak the parameters to generate a literary masterpiece.
    # ====== YOUR CODE: ======
    raise NotImplementedError()
    # ========================
    return start_seq, temperature


part1_q1 = r"""


"""

part1_q2 = r"""


"""

part1_q3 = r"""


"""

part1_q4 = r"""



"""
# ==============


# ==============
# Part 2 answers

PART2_CUSTOM_DATA_URL = None


def part2_vae_hyperparams():
    hypers = dict(
        batch_size=0,
        h_dim=0, z_dim=0, x_sigma2=0,
        learn_rate=0.0, betas=(0.0, 0.0),
    )
    # TODO: Tweak the hyperparameters to generate a former president.
    # ====== YOUR CODE: ======
    hypers['batch_size'] = 16
    hypers['h_dim']      = 1024
    hypers['z_dim']      = 64
    hypers['x_sigma2']   = 0.01
    hypers['learn_rate'] = 0.0001
    hypers['betas']      = (0, 0)


    # raise NotImplementedError()
    # ========================
    return hypers


part2_q1 = r"""

"""

part2_q2 = r"""

"""

# ==============

# ==============
# Part 3 answers

PART3_CUSTOM_DATA_URL = None


def part3_gan_hyperparams():
    hypers = dict(
        batch_size=0, z_dim=0,
        data_label=0, label_noise=0.0,
        discriminator_optimizer=dict(
            type='',  # Any name in nn.optim like SGD, Adam
            lr=0.0,
            # You an add extra args for the optimizer here
        ),
        generator_optimizer=dict(
            type='',  # Any name in nn.optim like SGD, Adam
            lr=0.0,
            # You an add extra args for the optimizer here
        ),
    )
    # TODO: Tweak the hyperparameters to train your GAN.
    # ====== YOUR CODE: ======
    lr = 0.0002
    hypers['batch_size'] = 4
    hypers['z_dim'] = 512
    hypers['data_label'] = 1
    hypers['label_noise'] = 0.2
    hypers['discriminator_optimizer']['type'] = 'Adam'
    hypers['discriminator_optimizer']['lr'] = lr
    hypers['generator_optimizer']['type'] = 'Adam'
    hypers['generator_optimizer']['lr'] = lr*5

    # raise NotImplementedError()
    # ========================
    return hypers


part3_q1 = r"""
During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences 
between distributions would not be useful, which may cause an unstable generator and mode collapse, thus in those 
situations gradients will be discarded.

"""

part3_q2 = r"""
1.The stop criteria for a GAN model is when it reaches Nash equilibrium.
Since we are using SGD it is not possible to reach Nash's equilibrium as the loss for both generator and discriminator 
oscillates
2.The discriminator loss is made of two parts, prediction of real images and fake images.
If the generator loss decrease, that means that the discriminator is getting worse at recognizing fake images.
The fact that the discriminator loss remains constant the discriminator is bad at recognizing real images from fake ones.
"""

part3_q3 = r"""
Images generated by VAE are blurred due to the maximum likelihood objective and it converged to the mean
distribution of the input images, mainly the face area.




"""

# ==============




















